---
title: Build completely private APIs in Snowflake
codetour: true
category: Learning
date: '2024-09-26'
description: TBD
image: /blog/snowflake-private-api/cover.png
author: Glenn Gillen
authorAvatar: /blog/glenn-gillen.jpg
---

{/* <!-- vale Microsoft.We = NO --> */}
{/* <!-- vale Microsoft.FirstPerson = NO --> */}
{/* <!-- vale ockam.h1-h6_sentence-case = NO --> */}
{/* <!-- vale Microsoft.Contractions = NO --> */}

If you've stored data in Snowflake then chances are you've also got an 
enterprise application that needs to access it, and a common way to do that is 
through an HTTP API. This is your organization's _private_ data though, so 
making it available through an API that is accessible on the _public_ internet 
is not the best approach.

In this guide I'm going to walk you through how to build, deploy, and host a 
_private_ custom API powered by Snowflake.

The API will not have an endpoint exposed to the Internet. Your application 
will, instead, access this API over private endpoints that are only available 
within your enterprise's VPC and other private environments.

The example will build a reporting endpoint (in Python) to return data from the 
[TPC-H](https://docs.snowflake.com/en/user-guide/sample-data-tpch) dataset 
already included in your Snowflake account. 

## Prerequisites

- [Snowflake](https://snowflake.com) Account in an AWS commercial region. 
    - Privileges necessary to create a user, database, warehouse, compute pool, 
repository, network rule, external access integration, and service in Snowflake.
    - Privileges necessary to access the tables in the 
`SNOWFLAKE_SAMPLE_DATA.TPCH_SF10` database and schema.
    - Access to run SQL in the Snowflake console or SnowSQL
- [GitHub](https://github.com/) Account with credits for Codespaces + basic 
experience using git.
- [Ockam](https://www.ockam.io/) Account to securely expose your private API
- Intermediate knowledge of Python

## Setup Snowflake

<CodeTour>

## !!steps Create a Warehouse

Using the Snowflake console or using SnowSQL run these commands

```sql ! tour
USE ROLE ACCOUNTADMIN;
CREATE WAREHOUSE DATA_API_WH WITH WAREHOUSE_SIZE='xsmall';
```

## !!steps 

Creating a warehouse requires `ACCOUNTADMIN` level permissions, so we'll first
switch to this role.

```sql ! tour
--!focus(1:1)
USE ROLE ACCOUNTADMIN;
CREATE WAREHOUSE DATA_API_WH WITH WAREHOUSE_SIZE='xsmall';
```

# !!steps 

We then create a new warehouse named `DATA_API_WH`, with a size of `xsmall`. 

```sql ! tour
USE ROLE ACCOUNTADMIN;
--!focus(1:1)
CREATE WAREHOUSE DATA_API_WH WITH WAREHOUSE_SIZE='xsmall';
```

## !!steps Create app role

```sql ! tour
USE ROLE ACCOUNTADMIN;
CREATE ROLE DATA_API_ROLE;

GRANT USAGE ON WAREHOUSE DATA_API_WH TO ROLE DATA_API_ROLE;
GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE_SAMPLE_DATA TO ROLE DATA_API_ROLE;

GRANT ROLE DATA_API_ROLE TO ROLE ACCOUNTADMIN;
```
</CodeTour>

## Setup development environment

The code in this guide comes from a lab that Brad Culberson and the team at 
Snowflake deliver to customers. We're going to use GitHub Codespaces as our 
development environment to make any changes we need to the code and to 
package it for deployment. Here's how to set it up:

* [Visit the `sfc-gh-bculberson/lab_data_api_python` repo](https://github.com/sfc-gh-bculberson/lab_data_api_python).
* Press `.`&hellip; _or_ click the green `<> Code` button choose `Codespaces` and click
`Create codespace on main`.

GitHub will then load a new Codespace environment with the code from this
repository.

## Python app code

In the `src/connector.py` file you'll find the code that defines the API
and generates responses for a number of endpoints that are served under the
`/connector` path. We're going to walk through just one of the endpoints as 
and example of how to retrieve data out of Snowflake and return it as JSON.


<CodeTour>

## !!steps Top 10 API endpoint

Within the `src/connector.py` find the line `@connector.route('/customers/top10')`,
the code block below it defines and API response that returns the top 10 customers
between a given date

```python ! tour
@connector.route('/customers/top10')
def customers_top10():
    # Validate arguments
    sdt_str = request.args.get('start_range') or '1995-01-01'
    edt_str = request.args.get('end_range') or '1995-03-31'
    try:
        sdt = datetime.datetime.strptime(sdt_str, dateformat)
        edt = datetime.datetime.strptime(edt_str, dateformat)
    except:
        abort(400, "Invalid start and/or end dates.")
    sql_string = '''
        SELECT
            o_custkey
          , SUM(o_totalprice) AS sum_totalprice
        FROM snowflake_sample_data.tpch_sf10.orders
        WHERE o_orderdate >= '{sdt}'
          AND o_orderdate <= '{edt}'
        GROUP BY o_custkey
        ORDER BY sum_totalprice DESC
        LIMIT 10
    '''
    sql = sql_string.format(sdt=sdt, edt=edt)
    try:
        res = conn.cursor(DictCursor).execute(sql)
        return make_response(jsonify(res.fetchall()))
    except:
        abort(500, "Error reading from Snowflake. Check the logs for details.")
```

## !!steps

This line uses a `Blueprint` route decorator to define a route at the path 
`/customers/top10` (within `app.py` everything in this file is mounted under
the sub-path of `/connector, so the final exposed path will end up being
`/connector/customers/top10`). The decorator tells Flask that the function that
immediately follows is what it should use to generate the response for 
any requests that come to this path.

```python ! tour
#!focus(1:1)
@connector.route('/customers/top10')
def customers_top10():
    # Validate arguments
    sdt_str = request.args.get('start_range') or '1995-01-01'
    edt_str = request.args.get('end_range') or '1995-03-31'
    try:
        sdt = datetime.datetime.strptime(sdt_str, dateformat)
        edt = datetime.datetime.strptime(edt_str, dateformat)
    except:
        abort(400, "Invalid start and/or end dates.")
    sql_string = '''
        SELECT
            o_custkey
          , SUM(o_totalprice) AS sum_totalprice
        FROM snowflake_sample_data.tpch_sf10.orders
        WHERE o_orderdate >= '{sdt}'
          AND o_orderdate <= '{edt}'
        GROUP BY o_custkey
        ORDER BY sum_totalprice DESC
        LIMIT 10
    '''
    sql = sql_string.format(sdt=sdt, edt=edt)
    try:
        res = conn.cursor(DictCursor).execute(sql)
        return make_response(jsonify(res.fetchall()))
    except:
        abort(500, "Error reading from Snowflake. Check the logs for details.")
```

## !!steps

We want to provide the functionality to return the top 10 customers within a 
given time range. To do that, we check the request parameters to find values 
for `start_range` and `end_range`, and if they are not set we  
set default values of `1995-01-01` and `1995-03-31` respectively.

Those variables are then parsed into `datetime` objects using `strptime`, if 
that fails we return an error stating the date formats are invalid.

```python ! tour
@connector.route('/customers/top10')
def customers_top10():
    # Validate arguments
    #!focus(1:7)
    sdt_str = request.args.get('start_range') or '1995-01-01'
    edt_str = request.args.get('end_range') or '1995-03-31'
    try:
        sdt = datetime.datetime.strptime(sdt_str, dateformat)
        edt = datetime.datetime.strptime(edt_str, dateformat)
    except:
        abort(400, "Invalid start and/or end dates.")
    sql_string = '''
        SELECT
            o_custkey
          , SUM(o_totalprice) AS sum_totalprice
        FROM snowflake_sample_data.tpch_sf10.orders
        WHERE o_orderdate >= '{sdt}'
          AND o_orderdate <= '{edt}'
        GROUP BY o_custkey
        ORDER BY sum_totalprice DESC
        LIMIT 10
    '''
    sql = sql_string.format(sdt=sdt, edt=edt)
    try:
        res = conn.cursor(DictCursor).execute(sql)
        return make_response(jsonify(res.fetchall()))
    except:
        abort(500, "Error reading from Snowflake. Check the logs for details.")
```

# !!steps

We then build a SQL statement that queries the `tpch_sf10.orders` table, 
returning rows where the `o_orderdate` is between the placeholders of our start 
and end range, we group by `o_custkey`, order the results descending by the 
`sum` of `o_totalprice` for each grouped customer, and finally `limit` to the 
top `10` rows.

The end result is a query that returns the top 10 customers by spend within a
date range.

```python ! tour
@connector.route('/customers/top10')
def customers_top10():
    # Validate arguments
    sdt_str = request.args.get('start_range') or '1995-01-01'
    edt_str = request.args.get('end_range') or '1995-03-31'
    try:
        sdt = datetime.datetime.strptime(sdt_str, dateformat)
        edt = datetime.datetime.strptime(edt_str, dateformat)
    except:
        abort(400, "Invalid start and/or end dates.")
    #!focus(1:11)        
    sql_string = '''
        SELECT
            o_custkey
          , SUM(o_totalprice) AS sum_totalprice
        FROM snowflake_sample_data.tpch_sf10.orders
        WHERE o_orderdate >= '{sdt}'
          AND o_orderdate <= '{edt}'
        GROUP BY o_custkey
        ORDER BY sum_totalprice DESC
        LIMIT 10
    '''
    sql = sql_string.format(sdt=sdt, edt=edt)
    try:
        res = conn.cursor(DictCursor).execute(sql)
        return make_response(jsonify(res.fetchall()))
    except:
        abort(500, "Error reading from Snowflake. Check the logs for details.")
```

# !!steps

The SQL string is then formatted by interpolating the `datetime` values for our
start and end dates into the appropriate place within string. We then use the
connection to Snowflake (`conn`, established earlier in the `connector.py` file)
to execute the query using a `DictCursor` which will return results with
column names as key values. 

Once we initialize the query we call the `fetchall()` method to return all 
of the results, which are then passed to `jsonify()` to serialize them to JSON,
before passing to `make_response()` to return the result to the client.

```python ! tour
@connector.route('/customers/top10')
def customers_top10():
    # Validate arguments
    sdt_str = request.args.get('start_range') or '1995-01-01'
    edt_str = request.args.get('end_range') or '1995-03-31'
    try:
        sdt = datetime.datetime.strptime(sdt_str, dateformat)
        edt = datetime.datetime.strptime(edt_str, dateformat)
    except:
        abort(400, "Invalid start and/or end dates.")
    sql_string = '''
        SELECT
            o_custkey
          , SUM(o_totalprice) AS sum_totalprice
        FROM snowflake_sample_data.tpch_sf10.orders
        WHERE o_orderdate >= '{sdt}'
          AND o_orderdate <= '{edt}'
        GROUP BY o_custkey
        ORDER BY sum_totalprice DESC
        LIMIT 10
    '''
    #!focus(1:6)            
    sql = sql_string.format(sdt=sdt, edt=edt)
    try:
        res = conn.cursor(DictCursor).execute(sql)
        return make_response(jsonify(res.fetchall()))
    except:
        abort(500, "Error reading from Snowflake. Check the logs for details.")
```
</CodeTour>

## Build & publish app container

In Codespaces click the hamburger menu icon (`â˜°`) > `Terminal` > `New Terminal`. 
Choose `Continue working in GitHub Codespaces` and select the smallest instance
possible (at the time of writing that's 2 cores).

<CodeTour>

## !!steps 

In the terminal window at the bottom of your Codespace run the `docker build` 
command to the right. It will build a container based on configuration in the 
current working directory (`.`) and then tag the resulting image with the name
`dataapi`.

```bash ! tour
docker build -t dataapi .
```

## !!steps Create the image registry

Using the Snowflake console or using SnowSQL run these commands:

```sql ! tour
USE ROLE ACCOUNTADMIN;
CREATE DATABASE API;

GRANT ALL ON DATABASE API TO ROLE DATA_API_ROLE;
CREATE SCHEMA IF NOT EXISTS API.PRIVATE;
GRANT ALL ON SCHEMA API.PRIVATE TO ROLE DATA_API_ROLE;

USE DATABASE API;
USE SCHEMA PRIVATE;
CREATE OR REPLACE IMAGE REPOSITORY API;
GRANT READ ON IMAGE REPOSITORY API TO ROLE DATA_API_ROLE;
SHOW IMAGE REPOSITORIES;
```

## !!steps 

Everything we're going to do here us going to require admin access, so we ensure
we're using the appropriate role.

```sql ! tour
--!focus(1:1)
USE ROLE ACCOUNTADMIN;
CREATE DATABASE API;

GRANT ALL ON DATABASE API TO ROLE DATA_API_ROLE;
CREATE SCHEMA IF NOT EXISTS API.PRIVATE;
GRANT ALL ON SCHEMA API.PRIVATE TO ROLE DATA_API_ROLE;

USE DATABASE API;
USE SCHEMA PRIVATE;
CREATE OR REPLACE IMAGE REPOSITORY API;
GRANT READ ON IMAGE REPOSITORY API TO ROLE DATA_API_ROLE;
SHOW IMAGE REPOSITORIES;
```

## !!steps 

Now we create a database named `API` specifically to contain our API related 
data. 

```sql ! tour
USE ROLE ACCOUNTADMIN;
--!focus(1:1)
CREATE DATABASE API;

GRANT ALL ON DATABASE API TO ROLE DATA_API_ROLE;
CREATE SCHEMA IF NOT EXISTS API.PRIVATE;
GRANT ALL ON SCHEMA API.PRIVATE TO ROLE DATA_API_ROLE;

USE DATABASE API;
USE SCHEMA PRIVATE;
CREATE OR REPLACE IMAGE REPOSITORY API;
GRANT READ ON IMAGE REPOSITORY API TO ROLE DATA_API_ROLE;
SHOW IMAGE REPOSITORIES;
```

## !!steps 

The `DATA_API_ROLE` role (created earlier in the Setup Snowflake steps) is
granted full access to the `API` database. Withing that database we then create 
a new schema named `PRIVATE` and grant full access on that schema too.

```sql ! tour
USE ROLE ACCOUNTADMIN;
CREATE DATABASE API;

--!focus(1:3)
GRANT ALL ON DATABASE API TO ROLE DATA_API_ROLE;
CREATE SCHEMA IF NOT EXISTS API.PRIVATE;
GRANT ALL ON SCHEMA API.PRIVATE TO ROLE DATA_API_ROLE;

USE DATABASE API;
USE SCHEMA PRIVATE;
CREATE OR REPLACE IMAGE REPOSITORY API;
GRANT READ ON IMAGE REPOSITORY API TO ROLE DATA_API_ROLE;
SHOW IMAGE REPOSITORIES;
```

## !!steps 

We ensure we're using the `API` database and the `PRIVATE` schema and create
an image repository named `API`, and then grant the `DATA_API_ROLE` full access 
to it.

```sql ! tour
USE ROLE ACCOUNTADMIN;
CREATE DATABASE API;

GRANT ALL ON DATABASE API TO ROLE DATA_API_ROLE;
CREATE SCHEMA IF NOT EXISTS API.PRIVATE;
GRANT ALL ON SCHEMA API.PRIVATE TO ROLE DATA_API_ROLE;

--!focus(1:4)
USE DATABASE API;
USE SCHEMA PRIVATE;
CREATE OR REPLACE IMAGE REPOSITORY API;
GRANT READ ON IMAGE REPOSITORY API TO ROLE DATA_API_ROLE;
SHOW IMAGE REPOSITORIES;
```

## !!steps 

Running the `SHOW IMAGE REPOSITORIES` command will return the details for the
repository we just created, it will something like:

{/* <!-- vale ockam.spelling = NO --> */}
{/* <!-- vale Microsoft.RangeFormat = NO --> */}
| created_on                    | name                | database_name | schema_name | repository_url                                                                                                  | owner     | owner_role_type | comment |
|-------------------------------|---------------------|---------------|-------------|-----------------------------------------------------------------------------------------------------------------|-----------|-----------------|---------|
| 2024-10-04 14:27:19.459 -0700 | API | API   | PRIVATE | orgname-acctname.registry.snowflakecomputing.com/api/private/api | DATA_API_ROLE | ROLE            |         |
{/* <!-- vale ockam.spelling = YES --> */}
{/* <!-- vale Microsoft.RangeFormat = YES --> */}

 Take note of the value of `repository_url`, as we're 
going to need it in a moment.

```sql ! tour
USE ROLE ACCOUNTADMIN;
CREATE DATABASE API;

GRANT ALL ON DATABASE API TO ROLE DATA_API_ROLE;
CREATE SCHEMA IF NOT EXISTS API.PRIVATE;
GRANT ALL ON SCHEMA API.PRIVATE TO ROLE DATA_API_ROLE;

USE DATABASE API;
USE SCHEMA PRIVATE;
CREATE OR REPLACE IMAGE REPOSITORY API;
GRANT READ ON IMAGE REPOSITORY API TO ROLE DATA_API_ROLE;
--!focus(1:1)
SHOW IMAGE REPOSITORIES;
```



## !!steps Push the container

Switch back to the terminal in your Codespace and run these commands, replacing
`<repository_url>` with the value of `repository_url` from the previous step.

```bash ! tour
docker login <repository_url>
docker build -t <repository_url>/dataapi .
docker push <repository_url>/dataapi
```

## !!steps 

Do stuff

```bash ! tour
# !focus(1:1)
docker login <repository_url>
docker build -t <repository_url>/dataapi .
docker push <repository_url>/dataapi
```

## !!steps 

Do stuff

```bash ! tour
docker login <repository_url>
# !focus(1:2)
docker build -t <repository_url>/dataapi .
docker push <repository_url>/dataapi
```
</CodeTour>

## Run in Snowflake

??

<CodeTour>
## !!steps Create the compute pool

```sql ! tour
USE ROLE ACCOUNTADMIN;

CREATE COMPUTE POOL API_POOl
  MIN_NODES = 1
  MAX_NODES = 5
  INSTANCE_FAMILY = CPU_X64_XS;

GRANT USAGE ON COMPUTE POOL API_POOl TO ROLE DATA_API_ROLE;
GRANT MONITOR ON COMPUTE POOL API_POOl TO ROLE DATA_API_ROLE;
```

## !!steps Create the app pool

```sql ! tour
USE ROLE DATA_API_ROLE;

CREATE SERVICE API.PRIVATE.API
 IN COMPUTE POOL API_POOl
 FROM SPECIFICATION
$$
spec:
  container:
  - name: api
    image: /api/private/api/dataapi:latest
    resources:
      requests:
        cpu: 0.5
        memory: 128M
      limits:
        cpu: 1
        memory: 256M
  endpoint:
  - name: api
    port: 8001
    public: false
$$
QUERY_WAREHOUSE = DATA_API_WH;
```

## !!steps 

```sql ! tour
CALL SYSTEM$GET_SERVICE_STATUS('api');
CALL SYSTEM$GET_SERVICE_LOGS('api.private.api', 0, 'api');
```

## !!steps 

```sql ! tour
SHOW ENDPOINTS IN SERVICE API;
```

## !!steps 

```sql ! tour
SHOW SERVICES;
```

</CodeTour>

## Setup Ockam

<CodeTour>

## !!steps 

```bash ! tour
# !focus(1:3)
curl --proto '=https' --tlsv1.2 -sSfL \
  https://install.command.ockam.io | \
  bash && source "$HOME/.ockam/env"
ockam enroll
ockam project ticket --usage-count 1 --expires-in 1h \
  --attribute snowflake-api-service-outlet \
  --relay snowflake-api-service-relay > ticket
```

## !!steps 

```bash ! tour
curl --proto '=https' --tlsv1.2 -sSfL \
  https://install.command.ockam.io | \
  bash && source "$HOME/.ockam/env"
# !focus(1:1)
ockam enroll
ockam project ticket --usage-count 1 --expires-in 1h \
  --attribute snowflake-api-service-outlet \
  --relay snowflake-api-service-relay > ticket
```

## !!steps 

```bash ! tour
curl --proto '=https' --tlsv1.2 -sSfL \
  https://install.command.ockam.io | \
  bash && source "$HOME/.ockam/env"
ockam enroll
# !focus(1:3)
ockam project ticket --usage-count 1 --expires-in 1h \
  --attribute snowflake-api-service-outlet \
  --relay snowflake-api-service-relay > ticket
```

## !!steps

```bash ! tour
ockam project show --jq '.egress_allow_list[]'
```

## !!steps 

```bash ! tour
docker pull ghcr.io/build-trust/ockam
docker tag ghcr.io/build-trust/ockam <repository_url>/ockam
docker push <repository_url>/ockam
```

## !!steps 

```sql ! tour
# Example
VALUE_LIST = ("4ed9ff5d-4953-4080-83c7-e69a3477a545.projects.orchestrator.ockam.io:443");
```

## !!steps 

```sql ! tour
USE ROLE ACCOUNTADMIN;

-- Update VALUE_LIST with ockam egress details
CREATE OR REPLACE NETWORK RULE OCKAM_OUT TYPE = 'HOST_PORT' MODE = 'EGRESS'
VALUE_LIST = ("<EGRESS_ALLOW_LIST>");

CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION OCKAM
ALLOWED_NETWORK_RULES = (OCKAM_OUT) ENABLED = true;

GRANT USAGE ON INTEGRATION OCKAM TO ROLE DATA_API_ROLE;

USE ROLE DATA_API_ROLE;

CREATE SERVICE API_OCKAM_OUTLET
IN COMPUTE POOL API_POOL
FROM SPECIFICATION
$$
spec:
  containers:
  - name: ockam-outlet
    image: /api/private/api/ockam:latest
    args:
      - node
      - create
      - --foreground
      - --enrollment-ticket
      - "<OCKAM_ENROLLMENT_TICKET>"
      - --node-config
      - |
        relay: snowflake-api-service-relay
        tcp-outlet:
          to: api.private.api.snowflakecomputing.internal:8001
          allow: snowflake-api-service-inlet
    env:
        OCKAM_DISABLE_UPGRADE_CHECK: true
        OCKAM_OPENTELEMETRY_EXPORT: false
$$
EXTERNAL_ACCESS_INTEGRATIONS = (OCKAM);
```

## !!steps 

```sql ! tour
CALL SYSTEM$GET_SERVICE_STATUS('API_OCKAM_OUTLET');
CALL SYSTEM$GET_SERVICE_LOGS('API_OCKAM_OUTLET', '0', 'ockam-outlet', 1000);
```

## !!steps 

```bash ! tour
docker run --rm -d  --name ockam-inlet -p 8001:8001 \
  ghcr.io/build-trust/ockam node create --foreground \
  --enrollment-ticket "$(ockam project ticket --usage-count 1 --expires-in 1h --attribute snowflake-api-service-inlet)" \
  --configuration "
    tcp-inlet:
      from: 0.0.0.0:8001
      via: snowflake-api-service-relay
      allow: snowflake-api-service-outlet
  "
```

## !!steps 

```bash ! tour
curl -X GET "http://localhost:8001/connector/customers/top10?start_range=1995-02-01&end_range=1995-02-14"
```

## !!steps 

```bash ! tour
curl -X GET "http://localhost:8001/snowpark/customers/top10?start_range=1995-02-01&end_range=1995-02-14"
```

## !!steps 

```bash ! tour
curl -X GET "http://localhost:8001/connector/clerk/000000002/yearly_sales/1995"
```

## !!steps 

```bash ! tour
curl -X GET "http://localhost:8001/connector/clerk/000000002/yearly_sales/1995"
```

## !!steps 

```sql ! tour
USE ROLE DATA_API_ROLE;
ALTER SERVICE API.PRIVATE.API SUSPEND;
```

## !!steps 

```sql ! tour
USE ROLE DATA_API_ROLE;

DROP SERVICE API.PRIVATE.API;
DROP SERVICE API_OCKAM_OUTLET;
```

## !!steps 

```sql ! tour
USE ROLE ACCOUNTADMIN;

DROP ROLE IF EXISTS DATA_API_ROLE;
DROP DATABASE IF EXISTS API;
DROP INTEGRATION IF EXISTS OCKAM;
DROP COMPUTE POOL IF EXISTS API_POOL;
DROP WAREHOUSE IF EXISTS DATA_API_WH;
```

</CodeTour>